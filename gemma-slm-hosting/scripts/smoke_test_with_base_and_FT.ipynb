{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1512,
     "status": "ok",
     "timestamp": 1768539889550,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "KTWiG8rA2_-e",
    "outputId": "14faca36-33f4-4648-d88a-aeb912cbaa2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/content/slm-hosting-playbook'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "rm -rf /content/slm-hosting-playbook\n",
    "git clone https://github.com/bh3r1th/slm-hosting-playbook.git /content/slm-hosting-playbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67911,
     "status": "ok",
     "timestamp": 1768534499993,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "kXB26LeN3dBA",
    "outputId": "095e1946-56a8-4f77-edfe-644fd2cdf6a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 79.1 MB/s eta 0:00:00\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "python -m pip -q install -U pip\n",
    "python -m pip -q install \"vllm\" \"jedi>=0.16\"\n",
    "python -m pip -q install \"requests==2.32.4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23713,
     "status": "ok",
     "timestamp": 1768534553615,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "2xfD0OJA30TK",
    "outputId": "5b59bc78-eaac-4956-e330-cb656e3b351e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2381,
     "status": "ok",
     "timestamp": 1768534563813,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "hpQTITx_38TC",
    "outputId": "9ba1d2fe-d048-4942-bfa9-54251136f26c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw------- 1 root root 1.1K Jan 14 09:19 /content/drive/MyDrive/slm-hosting-playbook-artifacts/live_backup/gemma-3-1b-it-lora-20260114-034307/adapter_config.json\n",
      "-rw------- 1 root root  25M Jan 14 09:19 /content/drive/MyDrive/slm-hosting-playbook-artifacts/live_backup/gemma-3-1b-it-lora-20260114-034307/adapter_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "export VLLM_HOST=0.0.0.0\n",
    "export VLLM_PORT_BASE=8000\n",
    "export VLLM_PORT_FT=8001\n",
    "export BASE_MODEL_ID=\"google/gemma-3-1b-it\"\n",
    "export FT_SERVED_MODEL_NAME=\"ft\"\n",
    "export GPU_MEMORY_UTILIZATION=0.90\n",
    "export MAX_MODEL_LEN=2048\n",
    "export TENSOR_PARALLEL_SIZE=1\n",
    "\n",
    "export ADAPTER_PATH=\"/content/drive/MyDrive/slm-hosting-playbook-artifacts/live_backup/gemma-3-1b-it-lora-20260114-034307\"\n",
    "\n",
    "# sanity: adapter files must exist\n",
    "ls -lah \"$ADAPTER_PATH\"/adapter_config.json \"$ADAPTER_PATH\"/adapter_model.safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1768535039118,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "eRVNyOUu5xTf",
    "outputId": "d0216f95-e07f-47c1-c00a-ae476bc863b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "# This script starts vLLM in OpenAI-compatible mode (v1/* endpoints) for local testing.\n",
      "set -euo pipefail\n",
      "\n",
      "script_dir=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\n",
      "\n",
      "if [ -f \"$script_dir/.env\" ]; then\n",
      "  declare -A env_overrides=()\n",
      "  while IFS= read -r line; do\n",
      "    case \"$line\" in\n",
      "      ''|\\#*) continue ;;\n",
      "    esac\n",
      "    if [[ \"$line\" =~ ^[[:space:]]*(export[[:space:]]+)?([A-Za-z_][A-Za-z0-9_]*)= ]]; then\n",
      "      key=\"${BASH_REMATCH[2]}\"\n",
      "      if printenv \"$key\" >/dev/null 2>&1; then\n",
      "        env_overrides[\"$key\"]=\"$(printenv \"$key\")\"\n",
      "      fi\n",
      "    fi\n",
      "  done < \"$script_dir/.env\"\n",
      "  set -a\n",
      "  # shellcheck disable=SC1090\n",
      "  . \"$script_dir/.env\"\n",
      "  set +a\n",
      "  for key in \"${!env_overrides[@]}\"; do\n",
      "    export \"$key=${env_overrides[$key]}\"\n",
      "  done\n",
      "fi\n",
      "\n",
      "eval \"$(python \"$script_dir/read_pointer.py\")\"\n",
      "\n",
      ": \"${GPU_MEMORY_UTILIZATION:=0.90}\"\n",
      ": \"${MAX_MODEL_LEN:=2048}\"\n",
      ": \"${TENSOR_PARALLEL_SIZE:=1}\"\n",
      ": \"${DTYPE:=auto}\"\n",
      "\n",
      "if [ -z \"${BASE_MODEL_ID:-}\" ]; then\n",
      "  echo \"Missing required BASE_MODEL_ID\" >&2\n",
      "  exit 1\n",
      "fi\n",
      "if [ -z \"${VLLM_HOST:-}\" ]; then\n",
      "  echo \"Missing required VLLM_HOST\" >&2\n",
      "  exit 1\n",
      "fi\n",
      "if [ -z \"${VLLM_PORT_BASE:-}\" ]; then\n",
      "  echo \"Missing required VLLM_PORT_BASE\" >&2\n",
      "  exit 1\n",
      "fi\n",
      "if ! [[ \"$VLLM_PORT_BASE\" =~ ^[0-9]+$ ]]; then\n",
      "  echo \"Invalid VLLM_PORT_BASE='$VLLM_PORT_BASE'\" >&2\n",
      "  exit 1\n",
      "fi\n",
      "\n",
      "echo \"Expected endpoints:\"\n",
      "echo \"http://$VLLM_HOST:$VLLM_PORT_BASE/v1/models\"\n",
      "echo \"http://$VLLM_HOST:$VLLM_PORT_BASE/v1/chat/completions\"\n",
      "\n",
      "python -m vllm.entrypoints.openai.api_server \\\n",
      "  --host \"$VLLM_HOST\" \\\n",
      "  --port \"$VLLM_PORT_BASE\" \\\n",
      "  --model \"$BASE_MODEL_ID\" \\\n",
      "  --gpu-memory-utilization \"$GPU_MEMORY_UTILIZATION\" \\\n",
      "  --max-model-len \"$MAX_MODEL_LEN\" \\\n",
      "  --tensor-parallel-size \"$TENSOR_PARALLEL_SIZE\" \\\n",
      "  --dtype \"$DTYPE\"\n",
      "==== last 400 lines of base log ====\n",
      "Expected endpoints:\n",
      "http://0.0.0.0:8000/v1/models\n",
      "http://0.0.0.0:8000/v1/chat/completions\n",
      "2026-01-16 03:40:55.367972: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-16 03:40:55.385399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768534855.406565    6277 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768534855.412961    6277 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768534855.429166    6277 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768534855.429203    6277 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768534855.429207    6277 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768534855.429210    6277 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-16 03:40:55.434008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m INFO 01-16 03:41:03 [api_server.py:1351] vLLM API server version 0.13.0\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m INFO 01-16 03:41:03 [utils.py:253] non-default args: {'host': '0.0.0.0', 'model': 'google/gemma-3-1b-it', 'max_model_len': 2048}\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 402, in hf_raise_for_status\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     response.raise_for_status()\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/requests/models.py\", line 1026, in raise_for_status\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     raise HTTPError(http_error_msg, response=self)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m \n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m The above exception was the direct cause of the following exception:\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m \n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 479, in cached_files\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     hf_hub_download(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     return fn(*args, **kwargs)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1007, in hf_hub_download\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     return _hf_hub_download_to_cache_dir(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1114, in _hf_hub_download_to_cache_dir\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1655, in _raise_on_head_call_error\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     raise head_call_error\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1543, in _get_metadata_or_catch_error\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     metadata = get_hf_file_metadata(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     return fn(*args, **kwargs)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1460, in get_hf_file_metadata\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     r = _request_wrapper(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m         ^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 283, in _request_wrapper\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     response = _request_wrapper(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m                ^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 307, in _request_wrapper\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     hf_raise_for_status(response)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 419, in hf_raise_for_status\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     raise _format(GatedRepoError, message, response) from e\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-6969b34f-3640b0b66b7f68925fbdd720;c3e218bc-48b2-4c9c-b48f-bfc7e69a0133)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m \n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m Cannot access gated repo for url https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json.\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m Access to model google/gemma-3-1b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m \n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m The above exception was the direct cause of the following exception:\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m \n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"<frozen runpy>\", line 88, in _run_code\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1469, in <module>\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     uvloop.run(run_server(args))\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 96, in run\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     return __asyncio.run(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m            ^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     return runner.run(main)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m            ^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     return self._loop.run_until_complete(task)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 48, in wrapper\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     return await main\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m            ^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1398, in run_server\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1417, in run_server_worker\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     async with build_async_engine_client(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     return await anext(self.gen)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 172, in build_async_engine_client\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     return await anext(self.gen)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 198, in build_async_engine_client_from_engine_args\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 1323, in create_engine_config\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     maybe_override_with_speculators(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/transformers_utils/config.py\", line 508, in maybe_override_with_speculators\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     config_dict, _ = PretrainedConfig.get_config_dict(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 662, in get_config_dict\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 721, in _get_config_dict\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     resolved_config_file = cached_file(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m                            ^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 322, in cached_file\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 543, in cached_files\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m     raise OSError(\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m OSError: You are trying to access a gated repo.\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m Make sure to have access to it at https://huggingface.co/google/gemma-3-1b-it.\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m 401 Client Error. (Request ID: Root=1-6969b34f-3640b0b66b7f68925fbdd720;c3e218bc-48b2-4c9c-b48f-bfc7e69a0133)\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m \n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m Cannot access gated repo for url https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json.\n",
      "\u001b[0;36m(APIServer pid=6277)\u001b[0;0m Access to model google/gemma-3-1b-it is restricted. You must have access to it and be authenticated to access it. Please log in.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "# show what the wrapper script is\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "sed -n '1,200p' scripts/start_base_vllm.sh\n",
    "\n",
    "# show last log\n",
    "echo \"==== last 400 lines of base log ====\"\n",
    "tail -n 400 /content/vllm_base.log || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3009,
     "status": "ok",
     "timestamp": 1768535963961,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "UaGdwwys50Q9",
    "outputId": "48064508-d0fd-45d4-e0da-6fb31995fd29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM pid=11161\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "pkill -f \"vllm.entrypoints.openai.api_server\" || true\n",
    "\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --port 8000 \\\n",
    "  --model google/gemma-3-1b-it \\\n",
    "  --gpu-memory-utilization 0.90 \\\n",
    "  --max-model-len 2048 \\\n",
    "  --tensor-parallel-size 1 \\\n",
    "  > /content/vllm_base.log 2>&1 &\n",
    "\n",
    "echo $! > /content/vllm_base.pid\n",
    "echo \"vLLM pid=$(cat /content/vllm_base.pid)\"\n",
    "sleep 3\n",
    "tail -n 120 /content/vllm_base.log || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1768535967198,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "OopknziN56WX",
    "outputId": "67c61fab-f723-4436-c8f1-dbc19371c0a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PID CMD\n",
      "  11161 python3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8000 --model google/gemma-3-1b-it --gpu-memory-utilization 0.90 --max-model-len 2048 --tensor-parallel-size 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "ps -p $(cat /content/vllm_base.pid) -o pid,cmd || true\n",
    "ss -ltnp | grep ':8000' || true\n",
    "tail -n 200 /content/vllm_base.log || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1768535975977,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "t1HA0hmF57yg",
    "outputId": "fbe5fbb0-5b14-4f6c-e161-56ad8f19efb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PID CMD\n",
      "  11161 python3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8000 --model google/gemma-3-1b-it --gpu-memory-utilization 0.90 --max-model-len 2048 --tensor-parallel-size 1\n",
      "2026-01-16 03:59:27.338135: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-16 03:59:27.355577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768535967.376717   11161 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768535967.383219   11161 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768535967.399547   11161 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768535967.399574   11161 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768535967.399577   11161 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768535967.399579   11161 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-16 03:59:27.404726: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[0;36m(APIServer pid=11161)\u001b[0;0m INFO 01-16 03:59:35 [api_server.py:1351] vLLM API server version 0.13.0\n",
      "\u001b[0;36m(APIServer pid=11161)\u001b[0;0m INFO 01-16 03:59:35 [utils.py:253] non-default args: {'host': '0.0.0.0', 'model': 'google/gemma-3-1b-it', 'max_model_len': 2048}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "ps -p $(cat /content/vllm_base.pid) -o pid,cmd || true\n",
    "ss -ltnp | grep ':8000' || true\n",
    "tail -n 200 /content/vllm_base.log || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70319,
     "status": "ok",
     "timestamp": 1768537607506,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "Ow6w6-7hDR10",
    "outputId": "77c58a9e-7f3c-4042-c0b0-454ae24fab8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE READY\n",
      "{\"object\":\"list\",\"data\":[{\"id\":\"google/gemma-3-1b-it\",\"object\":\"model\",\"created\":1768537607,\"owned_by\":\"vllm\",\"root\":\"google/gemma-3-1b-it\",\"parent\":null,\"max_model_len\":2048,\"permission\":[{\"id\":\"modelperm-89550fc31efe77fe\",\"object\":\"model_permission\",\"created\":1768537607,\"allow_create_engine\":false\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "export VLLM_HOST=0.0.0.0\n",
    "export VLLM_PORT_BASE=8000\n",
    "export BASE_MODEL_ID=\"google/gemma-3-1b-it\"\n",
    "\n",
    "# lower this a bit to be safe\n",
    "export GPU_MEMORY_UTILIZATION=0.60\n",
    "export MAX_MODEL_LEN=2048\n",
    "export TENSOR_PARALLEL_SIZE=1\n",
    "export DTYPE=auto\n",
    "\n",
    "nohup bash scripts/start_base_vllm.sh > /content/vllm_base.log 2>&1 & echo $! > /content/vllm_base.pid\n",
    "\n",
    "# wait until ready\n",
    "for i in $(seq 1 180); do\n",
    "  curl -sf http://127.0.0.1:8000/v1/models >/dev/null && echo \"BASE READY\" && break\n",
    "  sleep 2\n",
    "done\n",
    "\n",
    "curl -s http://127.0.0.1:8000/v1/models | head -c 300; echo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1768538939907,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "Fzbh5PPBInQ-",
    "outputId": "52439383-b5c3-4d65-ade1-3c885a05e70d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"google/gemma-3-1b-it\",\"object\":\"model\",\"created\":1768538939,\"owned_by\":\"vllm\",\"root\":\"google/gemma-3-1b-it\",\"parent\":null,\"max_model_len\":2048,\"permission\":[{\"id\":\"modelperm-b528e7d8520cd7d1\",\"object\":\"model_permission\",\"created\":1768538939,\"allow_create_engine\":false\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "curl -s http://127.0.0.1:8000/v1/models | head -c 300; echo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1768539360442,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "qah7Ai-hJPPT",
    "outputId": "11d6d198-e250-4b33-f68e-41728da8264a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_SMOKE_OK: OK.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "curl -sf http://127.0.0.1:8000/v1/models >/dev/null\n",
    "python - <<'PY'\n",
    "import requests\n",
    "url=\"http://127.0.0.1:8000/v1/chat/completions\"\n",
    "payload={\n",
    "  \"model\":\"google/gemma-3-1b-it\",\n",
    "  \"messages\":[{\"role\":\"user\",\"content\":\"Say 'OK' and nothing else.\"}],\n",
    "  \"temperature\":0.0,\n",
    "  \"max_tokens\":16\n",
    "}\n",
    "r=requests.post(url, json=payload, timeout=120)\n",
    "r.raise_for_status()\n",
    "print(\"BASE_SMOKE_OK:\", r.json()[\"choices\"][0][\"message\"][\"content\"])\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1768539907330,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "Qb87S482KVBy",
    "outputId": "79f36088-e017-4114-ee4d-ef5adca5420e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: Hello there!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "python scripts/smoke_test.py --mode base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1768540208206,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "ANB-IaHANfOa",
    "outputId": "ccd89f3c-1de6-41c9-cfcd-51f05cb20860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LISTEN 0      2048         0.0.0.0:8000       0.0.0.0:*    users:((\"python3\",pid=20204,fd=33))         \n",
      "root       20204  0.9  2.2 14227688 1929848 ?    Sl   04:25   0:24 python3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8000 --model google/gemma-3-1b-it --gpu-memory-utilization 0.60 --max-model-len 2048 --tensor-parallel-size 1 --dtype auto\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "ss -ltnp | egrep ':8000|:8001' || true\n",
    "ps aux | grep -E \"vllm.*api_server\" | grep -v grep || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1768540230122,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "wC8b0M-BNi4s",
    "outputId": "3f027aeb-16eb-44d8-c4af-5b96044be1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: Hello there!\n",
      "SMOKE_TEST_BASE_ONLY_OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "python scripts/smoke_test.py --mode base\n",
    "echo \"SMOKE_TEST_BASE_ONLY_OK\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2188,
     "status": "ok",
     "timestamp": 1768541075800,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "2nvqyGSRPhzd",
    "outputId": "7ce749b4-497d-403f-db0e-79b07fb45193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 16 05:24:35 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0             69W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "pkill -f \"vllm.entrypoints.openai.api_server\" || true\n",
    "pkill -f \"vllm.*api_server\" || true\n",
    "sleep 2\n",
    "nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86404,
     "status": "ok",
     "timestamp": 1768541183852,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "UevQPmgfQ0eb",
    "outputId": "8e84856c-7078-4834-f9a6-0b514548596c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT READY\n",
      "{\"object\":\"list\",\"data\":[{\"id\":\"ft\",\"object\":\"model\",\"created\":1768541183,\"owned_by\":\"vllm\",\"root\":\"google/gemma-3-1b-it\",\"parent\":null,\"max_model_len\":2048,\"permission\":[{\"id\":\"modelperm-b20757271758221a\",\"object\":\"model_permission\",\"created\":1768541183,\"allow_create_engine\":false,\"allow_sampling\":\n",
      "W0000 00:00:1768541125.014350   36376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768541125.014378   36376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768541125.014392   36376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768541125.014394   36376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:32 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='google/gemma-3-1b-it', speculative_config=None, tokenizer='google/gemma-3-1b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=ft, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:33 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.28.0.12:49609 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:33 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:34 [gpu_model_runner.py:3562] Starting to load model google/gemma-3-1b-it...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:34 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:35 [weight_utils.py:527] No model.safetensors.index.json found in remote.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m \rLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m \rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.78it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m \rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.78it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:36 [default_loader.py:308] Loading weights took 0.60 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:36 [punica_selector.py:20] Using PunicaWrapperGPU.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:37 [gpu_model_runner.py:3659] Model loading took 1.9387 GiB memory and 2.183459 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:49 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/1695a4e9cf/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:49 [backends.py:703] Dynamo bytecode transform time: 11.56 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:58 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 2048) from the cache, took 2.177 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:25:58 [monitor.py:34] torch.compile takes 13.74 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:26:00 [gpu_worker.py:375] Available KV cache memory: 13.44 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m WARNING 01-16 05:26:00 [kv_cache_utils.py:1033] Add 2 padding layers, may waste at most 9.09% KV cache memory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:26:00 [kv_cache_utils.py:1291] GPU KV cache size: 503,184 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:26:00 [kv_cache_utils.py:1296] Maximum concurrency for 2,048 tokens per request: 244.06x\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m \rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/102 [00:00<?, ?it/s]\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m WARNING 01-16 05:26:01 [utils.py:250] Using default LoRA kernel configs\n",
      "\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|          | 1/102 [00:00<00:27,  3.67it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 3/102 [00:00<00:13,  7.47it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▍         | 5/102 [00:00<00:10,  9.08it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|▋         | 7/102 [00:00<00:09,  9.71it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 9/102 [00:00<00:09, 10.28it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 11/102 [00:01<00:08, 10.69it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|█▎        | 13/102 [00:01<00:08, 10.97it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|█▍        | 15/102 [00:01<00:07, 11.09it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 17/102 [00:01<00:07, 11.17it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▊        | 19/102 [00:01<00:07, 11.32it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 21/102 [00:02<00:07, 11.42it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 23/102 [00:02<00:06, 11.57it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▍       | 25/102 [00:02<00:06, 11.61it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 27/102 [00:02<00:06, 11.63it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 29/102 [00:02<00:06, 11.63it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|███       | 31/102 [00:02<00:06, 11.76it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 33/102 [00:03<00:05, 11.79it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 35/102 [00:03<00:05, 11.51it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 37/102 [00:03<00:05, 11.54it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  38%|███▊      | 39/102 [00:03<00:05, 11.54it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 41/102 [00:03<00:05, 11.47it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 43/102 [00:03<00:05, 11.44it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 45/102 [00:04<00:04, 11.46it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 47/102 [00:04<00:04, 11.45it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|████▊     | 49/102 [00:04<00:04, 11.43it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 51/102 [00:04<00:04, 11.42it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|█████▏    | 53/102 [00:04<00:04, 11.42it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 55/102 [00:04<00:04, 11.51it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 57/102 [00:05<00:03, 11.61it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 59/102 [00:05<00:03, 11.73it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|█████▉    | 61/102 [00:05<00:03, 11.78it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  62%|██████▏   | 63/102 [00:05<00:03, 11.70it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 65/102 [00:05<00:03, 11.76it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 67/102 [00:05<00:03, 11.46it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 69/102 [00:06<00:02, 11.23it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|██████▉   | 71/102 [00:06<00:02, 11.18it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 73/102 [00:06<00:02, 11.25it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 75/102 [00:06<00:02, 11.27it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 77/102 [00:06<00:02, 11.30it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 79/102 [00:07<00:02, 11.34it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 81/102 [00:07<00:01, 11.32it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████▏ | 83/102 [00:07<00:01, 11.33it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 85/102 [00:07<00:01, 11.39it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|████████▌ | 87/102 [00:07<00:01, 11.29it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|████████▋ | 89/102 [00:07<00:01, 10.89it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 91/102 [00:08<00:01, 10.96it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 93/102 [00:08<00:00, 10.92it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|█████████▎| 95/102 [00:08<00:00, 11.04it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▌| 97/102 [00:08<00:00, 10.94it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 99/102 [00:08<00:00, 11.01it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|█████████▉| 101/102 [00:09<00:00, 10.43it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 102/102 [00:09<00:00, 11.15it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m \rCapturing CUDA graphs (decode, FULL):   0%|          | 0/70 [00:00<?, ?it/s]\rCapturing CUDA graphs (decode, FULL):   1%|▏         | 1/70 [00:00<00:12,  5.64it/s]\rCapturing CUDA graphs (decode, FULL):   4%|▍         | 3/70 [00:00<00:07,  9.27it/s]\rCapturing CUDA graphs (decode, FULL):   7%|▋         | 5/70 [00:00<00:06, 10.37it/s]\rCapturing CUDA graphs (decode, FULL):  10%|█         | 7/70 [00:00<00:05, 11.00it/s]\rCapturing CUDA graphs (decode, FULL):  13%|█▎        | 9/70 [00:00<00:05, 11.34it/s]\rCapturing CUDA graphs (decode, FULL):  16%|█▌        | 11/70 [00:01<00:05, 11.70it/s]\rCapturing CUDA graphs (decode, FULL):  19%|█▊        | 13/70 [00:01<00:04, 11.98it/s]\rCapturing CUDA graphs (decode, FULL):  21%|██▏       | 15/70 [00:01<00:04, 12.23it/s]\rCapturing CUDA graphs (decode, FULL):  24%|██▍       | 17/70 [00:01<00:04, 12.30it/s]\rCapturing CUDA graphs (decode, FULL):  27%|██▋       | 19/70 [00:01<00:04, 12.46it/s]\rCapturing CUDA graphs (decode, FULL):  30%|███       | 21/70 [00:01<00:03, 12.46it/s]\rCapturing CUDA graphs (decode, FULL):  33%|███▎      | 23/70 [00:01<00:03, 12.49it/s]\rCapturing CUDA graphs (decode, FULL):  36%|███▌      | 25/70 [00:02<00:03, 12.33it/s]\rCapturing CUDA graphs (decode, FULL):  39%|███▊      | 27/70 [00:02<00:03, 12.22it/s]\rCapturing CUDA graphs (decode, FULL):  41%|████▏     | 29/70 [00:02<00:03, 12.21it/s]\rCapturing CUDA graphs (decode, FULL):  44%|████▍     | 31/70 [00:02<00:03, 12.09it/s]\rCapturing CUDA graphs (decode, FULL):  47%|████▋     | 33/70 [00:02<00:03, 12.05it/s]\rCapturing CUDA graphs (decode, FULL):  50%|█████     | 35/70 [00:02<00:02, 11.95it/s]\rCapturing CUDA graphs (decode, FULL):  53%|█████▎    | 37/70 [00:03<00:02, 11.93it/s]\rCapturing CUDA graphs (decode, FULL):  56%|█████▌    | 39/70 [00:03<00:02, 11.93it/s]\rCapturing CUDA graphs (decode, FULL):  59%|█████▊    | 41/70 [00:03<00:02, 11.87it/s]\rCapturing CUDA graphs (decode, FULL):  61%|██████▏   | 43/70 [00:03<00:02, 11.89it/s]\rCapturing CUDA graphs (decode, FULL):  64%|██████▍   | 45/70 [00:03<00:02, 11.96it/s]\rCapturing CUDA graphs (decode, FULL):  67%|██████▋   | 47/70 [00:03<00:01, 12.01it/s]\rCapturing CUDA graphs (decode, FULL):  70%|███████   | 49/70 [00:04<00:01, 12.05it/s]\rCapturing CUDA graphs (decode, FULL):  73%|███████▎  | 51/70 [00:04<00:01, 12.08it/s]\rCapturing CUDA graphs (decode, FULL):  76%|███████▌  | 53/70 [00:04<00:01, 12.10it/s]\rCapturing CUDA graphs (decode, FULL):  79%|███████▊  | 55/70 [00:04<00:01, 12.12it/s]\rCapturing CUDA graphs (decode, FULL):  81%|████████▏ | 57/70 [00:04<00:01, 12.24it/s]\rCapturing CUDA graphs (decode, FULL):  84%|████████▍ | 59/70 [00:04<00:00, 12.29it/s]\rCapturing CUDA graphs (decode, FULL):  87%|████████▋ | 61/70 [00:05<00:00, 12.23it/s]\rCapturing CUDA graphs (decode, FULL):  90%|█████████ | 63/70 [00:05<00:00, 12.19it/s]\rCapturing CUDA graphs (decode, FULL):  93%|█████████▎| 65/70 [00:05<00:00, 12.07it/s]\rCapturing CUDA graphs (decode, FULL):  96%|█████████▌| 67/70 [00:05<00:00, 12.10it/s]\rCapturing CUDA graphs (decode, FULL):  99%|█████████▊| 69/70 [00:05<00:00, 12.17it/s]\rCapturing CUDA graphs (decode, FULL): 100%|██████████| 70/70 [00:05<00:00, 12.01it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:26:16 [gpu_model_runner.py:4587] Graph capturing finished in 16 secs, took 1.11 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=36376)\u001b[0;0m INFO 01-16 05:26:16 [core.py:259] init engine (profile, create kv cache, warmup model) took 39.20 seconds\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:20 [api_server.py:1099] Supported tasks: ['generate']\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:20 [serving_models.py:168] Loaded new LoRA adapter: name 'ft', path '/content/drive/MyDrive/slm-hosting-playbook-artifacts/live_backup/gemma-3-1b-it-lora-20260114-034307'\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m WARNING 01-16 05:26:20 [model.py:1487] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:20 [serving_responses.py:201] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [serving_chat.py:137] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [serving_completion.py:77] Using default completion sampling params from model: {'top_k': 64, 'top_p': 0.95}\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [serving_chat.py:137] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [api_server.py:1425] Starting vLLM API server 0 on http://0.0.0.0:8001\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:38] Available routes are:\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /docs, Methods: GET, HEAD\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /redoc, Methods: GET, HEAD\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /tokenize, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /detokenize, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /inference/v1/generate, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /pause, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /resume, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /is_paused, Methods: GET\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /metrics, Methods: GET\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /health, Methods: GET\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /load, Methods: GET\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/models, Methods: GET\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /version, Methods: GET\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/responses, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/messages, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/chat/completions, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/completions, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/audio/translations, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /ping, Methods: GET\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /ping, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /invocations, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /classify, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /score, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/score, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /rerank, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v1/rerank, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /v2/rerank, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO 01-16 05:26:21 [launcher.py:46] Route: /pooling, Methods: POST\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO:     Started server process [36218]\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO:     Waiting for application startup.\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO:     Application startup complete.\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO:     127.0.0.1:52654 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[0;36m(APIServer pid=36218)\u001b[0;0m INFO:     127.0.0.1:52668 - \"GET /v1/models HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "export VLLM_HOST=0.0.0.0\n",
    "export VLLM_PORT_FT=8001\n",
    "export BASE_MODEL_ID=\"google/gemma-3-1b-it\"\n",
    "export ADAPTER_PATH=\"/content/drive/MyDrive/slm-hosting-playbook-artifacts/live_backup/gemma-3-1b-it-lora-20260114-034307\"\n",
    "\n",
    "# hard anti-OOM settings\n",
    "export GPU_MEMORY_UTILIZATION=0.45\n",
    "export MAX_MODEL_LEN=2048\n",
    "export MAX_NUM_SEQS=8\n",
    "export ENFORCE_EAGER=1\n",
    "\n",
    "nohup bash scripts/start_ft_vllm.sh > /content/vllm_ft.log 2>&1 & echo $! > /content/vllm_ft.pid\n",
    "\n",
    "# wait until ready\n",
    "for i in $(seq 1 180); do\n",
    "  curl -sf http://127.0.0.1:8001/v1/models >/dev/null && echo \"FT READY\" && break\n",
    "  sleep 2\n",
    "done\n",
    "\n",
    "curl -s http://127.0.0.1:8001/v1/models | head -c 300; echo\n",
    "tail -n 80 /content/vllm_ft.log || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2532,
     "status": "ok",
     "timestamp": 1768541248729,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "EaI9vqJwRUR8",
    "outputId": "f14112ce-72be-4b3b-d7c0-45c749ee515b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT: Hello! 😊\n",
      "SMOKE_TEST_FT_ONLY_OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "python scripts/smoke_test.py --mode ft\n",
    "echo \"SMOKE_TEST_FT_ONLY_OK\"\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPtkB7AJhaYNQijfRfjz9qp",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
