{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12109,
     "status": "ok",
     "timestamp": 1768705566398,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "s6WDoo7U_j-u",
    "outputId": "9b73326d-9402-4551-9b0c-fba192e0179e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1414,
     "status": "ok",
     "timestamp": 1768705569142,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "DSfzsM9c_v40",
    "outputId": "5e28bb9e-b2c5-448f-9811-1496d1646f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'slm-hosting-playbook'...\n",
      "remote: Enumerating objects: 314, done.\u001b[K\n",
      "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
      "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
      "remote: Total 314 (delta 81), reused 161 (delta 73), pack-reused 142 (from 2)\u001b[K\n",
      "Receiving objects: 100% (314/314), 6.81 MiB | 19.88 MiB/s, done.\n",
      "Resolving deltas: 100% (96/96), done.\n"
     ]
    }
   ],
   "source": [
    "!cd /content\n",
    "!rm -rf slm-hosting-playbook || true\n",
    "!git clone https://github.com/bh3r1th/slm-hosting-playbook.git slm-hosting-playbook\n",
    "!cd slm-hosting-playbook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1768705575183,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "TcGQxrIu_7eW",
    "outputId": "c2ede22a-723b-48d6-8486-12da06e43350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ .env ready at gemma-slm-hosting/.env\n",
      "Adapter path: /content/drive/MyDrive/slm-hosting-playbook-artifacts/live_backup/gemma-3-1b-it-lora-20260114-034307\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "cp -f .env.example .env\n",
    "\n",
    "# (Optional) override adapter path here if needed:\n",
    "# ADAPTER_PATH=\"/content/drive/MyDrive/....\"\n",
    "# sed -i \"s|^ADAPTER_PATH=.*|ADAPTER_PATH=${ADAPTER_PATH}|g\" .env\n",
    "\n",
    "# Quick sanity checks (read ADAPTER_PATH from .env)\n",
    "ADAPTER_PATH=\"$(grep -E '^ADAPTER_PATH=' .env | head -n 1 | cut -d= -f2-)\"\n",
    "test -d \"$ADAPTER_PATH\"\n",
    "test -f \"$ADAPTER_PATH/adapter_config.json\"\n",
    "test -f \"$ADAPTER_PATH/adapter_model.safetensors\"\n",
    "\n",
    "echo \"✅ .env ready at gemma-slm-hosting/.env\"\n",
    "echo \"Adapter path: $ADAPTER_PATH\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1768705578325,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "Wxq8UPRnBTpf",
    "outputId": "015eba04-2f8f-4d29-f40c-bc42ba1644a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Effective endpoints ==\n",
      "BASE_API_URL=http://127.0.0.1:8000/v1\n",
      "FT_API_URL=http://127.0.0.1:8001/v1\n",
      "GPU_MEMORY_UTILIZATION=0.90\n",
      "MAX_NUM_SEQS=128\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "# Ensure BASE_API_URL / FT_API_URL include /v1\n",
    "if grep -qE '^BASE_API_URL=http://127\\.0\\.0\\.1:8000$' .env; then\n",
    "  sed -i 's|^BASE_API_URL=http://127\\.0\\.0\\.1:8000$|BASE_API_URL=http://127.0.0.1:8000/v1|g' .env\n",
    "fi\n",
    "if grep -qE '^FT_API_URL=http://127\\.0\\.0\\.1:8001$' .env; then\n",
    "  sed -i 's|^FT_API_URL=http://127\\.0\\.0\\.1:8001$|FT_API_URL=http://127.0.0.1:8001/v1|g' .env\n",
    "fi\n",
    "\n",
    "# OPTIONAL: Colab VRAM safety (uncomment if you see OOM)\n",
    "# sed -i 's|^MAX_NUM_SEQS=.*|MAX_NUM_SEQS=8|g' .env\n",
    "# sed -i 's|^GPU_MEMORY_UTILIZATION=.*|GPU_MEMORY_UTILIZATION=0.85|g' .env\n",
    "\n",
    "echo \"== Effective endpoints ==\"\n",
    "grep -E '^(BASE_API_URL|FT_API_URL|MAX_NUM_SEQS|GPU_MEMORY_UTILIZATION)=' .env || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71537,
     "status": "ok",
     "timestamp": 1768705653740,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "Nn7JZvajBWBl",
    "outputId": "671b2e2e-4762-49e6-a6e0-07fb6319cfc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 23.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.3\n",
      "Collecting vllm\n",
      "  Downloading vllm-0.13.0-cp38-abi3-manylinux_2_31_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm) (2025.11.3)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm) (6.2.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm) (5.9.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm) (0.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vllm) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm) (4.67.1)\n",
      "Collecting blake3 (from vllm)\n",
      "  Downloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: transformers<5,>=4.56.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.57.3)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.22.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm) (5.29.5)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.123.10)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm) (3.13.3)\n",
      "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.14.0)\n",
      "Requirement already satisfied: pydantic>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.12.3)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm) (11.3.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.12.0)\n",
      "Collecting lm-format-enforcer==0.11.3 (from vllm)\n",
      "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<1.4.0,>=1.3.0 (from vllm)\n",
      "  Downloading llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines_core==0.2.11 (from vllm)\n",
      "  Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting diskcache==5.6.3 (from vllm)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lark==1.2.2 (from vllm)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.27 (from vllm)\n",
      "  Downloading xgrammar-0.1.27-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.15.0)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (3.20.2)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Downloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (26.2.1)\n",
      "Collecting msgspec (from vllm)\n",
      "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting gguf>=0.17.0 (from vllm)\n",
      "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.8.5 (from mistral_common[image]>=1.8.5->vllm)\n",
      "  Downloading mistral_common-1.8.8-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.12.0.88)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm) (6.0.3)\n",
      "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.17.0)\n",
      "Collecting setuptools<81.0.0,>=77.0.3 (from vllm)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm) (0.8.1)\n",
      "Collecting compressed-tensors==0.12.2 (from vllm)\n",
      "  Downloading compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.20.0 (from vllm)\n",
      "  Downloading depyf-0.20.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm) (3.1.2)\n",
      "Requirement already satisfied: watchfiles in /usr/local/lib/python3.12/dist-packages (from vllm) (1.1.1)\n",
      "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm) (4.0.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm) (1.16.3)\n",
      "Collecting ninja (from vllm)\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pybase64 (from vllm)\n",
      "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting cbor2 (from vllm)\n",
      "  Downloading cbor2-5.8.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting ijson (from vllm)\n",
      "  Downloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
      "Collecting setproctitle (from vllm)\n",
      "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
      "Collecting openai-harmony>=0.0.3 (from vllm)\n",
      "  Downloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting anthropic==0.71.0 (from vllm)\n",
      "  Downloading anthropic-0.71.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting model-hosting-container-standards<1.0.0,>=0.1.9 (from vllm)\n",
      "  Downloading model_hosting_container_standards-0.1.13-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: mcp in /usr/local/lib/python3.12/dist-packages (from vllm) (1.25.0)\n",
      "Collecting numba==0.61.2 (from vllm)\n",
      "  Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchaudio==2.9.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision==0.24.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.24.0+cu126)\n",
      "Collecting flashinfer-python==0.5.3 (from vllm)\n",
      "  Downloading flashinfer_python-0.5.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.17.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (1.3.1)\n",
      "Collecting loguru (from compressed-tensors==0.12.2->vllm)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting astor (from depyf==0.20.0->vllm)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.20.0->vllm) (0.3.8)\n",
      "Collecting apache-tvm-ffi<0.2,>=0.1 (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading apache_tvm_ffi-0.1.8.post2-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.3->vllm) (8.3.1)\n",
      "Collecting nvidia-cudnn-frontend>=1.13.0 (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading nvidia_cudnn_frontend-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting nvidia-cutlass-dsl>=4.2.1 (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading nvidia_cutlass_dsl-4.3.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.3->vllm) (13.590.44)\n",
      "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.3->vllm) (25.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.3->vllm) (0.9.0)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
      "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->vllm) (3.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic==0.71.0->vllm) (3.11)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (0.16.0)\n",
      "Collecting jmespath (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: starlette>=0.49.1 in /usr/local/lib/python3.12/dist-packages (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm) (0.50.0)\n",
      "Collecting supervisor>=4.2.0 (from model-hosting-container-standards<1.0.0,>=0.1.9->vllm)\n",
      "  Downloading supervisor-4.3.0-py2.py3-none-any.whl.metadata (87 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.56.0->vllm) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.56.0->vllm) (0.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers<5,>=4.56.0->vllm) (1.2.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.0.4)\n",
      "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cli-0.0.20-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.21)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.40.0)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.1)\n",
      "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rich_toolkit-0.17.1-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cloud_cli-0.11.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.49.0)\n",
      "Collecting fastar>=0.8.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastar-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->vllm) (3.0.3)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (4.26.0)\n",
      "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\n",
      "  Downloading pydantic_extra_types-2.11.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.30.0)\n",
      "Requirement already satisfied: cuda-python>=12.8 in /usr/local/lib/python3.12/dist-packages (from nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm) (12.9.5)\n",
      "Requirement already satisfied: cuda-bindings~=12.9.5 in /usr/local/lib/python3.12/dist-packages (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm) (12.9.5)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings~=12.9.5->cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm) (1.3.3)\n",
      "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.2)\n",
      "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (13.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2.5.0)\n",
      "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->vllm) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.2.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.22.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.22.0)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp->vllm) (0.4.3)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp->vllm) (2.12.0)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp->vllm) (2.10.1)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp->vllm) (3.1.2)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp->vllm) (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm) (2.23)\n",
      "Downloading vllm-0.13.0-cp38-abi3-manylinux_2_31_x86_64.whl (474.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.9/474.9 MB 48.5 MB/s  0:00:06\n",
      "Downloading anthropic-0.71.0-py3-none-any.whl (355 kB)\n",
      "Downloading compressed_tensors-0.12.2-py3-none-any.whl (183 kB)\n",
      "Downloading depyf-0.20.0-py3-none-any.whl (39 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading flashinfer_python-0.5.3-py3-none-any.whl (7.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 109.3 MB/s  0:00:00\n",
      "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Downloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n",
      "Downloading numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 119.9 MB/s  0:00:00\n",
      "Downloading outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 80.2 MB/s  0:00:00\n",
      "Downloading xgrammar-0.1.27-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.9/8.9 MB 124.5 MB/s  0:00:00\n",
      "Downloading apache_tvm_ffi-0.1.8.post2-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (2.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 62.3 MB/s  0:00:00\n",
      "Downloading llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 107.7 MB/s  0:00:00\n",
      "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.4/42.4 MB 97.2 MB/s  0:00:00\n",
      "Downloading model_hosting_container_standards-0.1.13-py3-none-any.whl (105 kB)\n",
      "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 72.7 MB/s  0:00:00\n",
      "Downloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
      "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
      "Downloading fastapi_cli-0.0.20-py3-none-any.whl (12 kB)\n",
      "Downloading fastapi_cloud_cli-0.11.0-py3-none-any.whl (26 kB)\n",
      "Downloading fastar-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (821 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 821.6/821.6 kB 44.8 MB/s  0:00:00\n",
      "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading mistral_common-1.8.8-py3-none-any.whl (6.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 100.1 MB/s  0:00:00\n",
      "Downloading nvidia_cudnn_frontend-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 95.0 MB/s  0:00:00\n",
      "Downloading nvidia_cutlass_dsl-4.3.5-cp312-cp312-manylinux_2_28_x86_64.whl (58.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.6/58.6 MB 93.3 MB/s  0:00:00\n",
      "Downloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 102.3 MB/s  0:00:00\n",
      "Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading pydantic_extra_types-2.11.0-py3-none-any.whl (74 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 159.1 MB/s  0:00:00\n",
      "Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl (72.4 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.4/72.4 MB 94.2 MB/s  0:00:00\n",
      "Downloading rich_toolkit-0.17.1-py3-none-any.whl (31 kB)\n",
      "Downloading rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (959 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 959.8/959.8 kB 60.4 MB/s  0:00:00\n",
      "Downloading supervisor-4.3.0-py2.py3-none-any.whl (320 kB)\n",
      "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading blake3-1.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (388 kB)\n",
      "Downloading cbor2-5.8.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (285 kB)\n",
      "Downloading ijson-3.4.0.post0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (149 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
      "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Downloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl (10 kB)\n",
      "Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
      "Installing collected packages: supervisor, setuptools, setproctitle, rignore, pycountry, pybase64, partial-json-parser, outlines_core, nvidia-cudnn-frontend, ninja, msgspec, loguru, llvmlite, llguidance, lark, jmespath, interegular, ijson, gguf, fastar, dnspython, diskcache, cbor2, blake3, astor, apache-tvm-ffi, numba, email-validator, depyf, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai-harmony, nvidia-cutlass-dsl, lm-format-enforcer, anthropic, ray, model-hosting-container-standards, flashinfer-python, fastapi-cloud-cli, fastapi-cli, xgrammar, mistral_common, compressed-tensors, vllm\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.2.0\n",
      "    Uninstalling setuptools-75.2.0:\n",
      "      Successfully uninstalled setuptools-75.2.0\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.43.0\n",
      "    Uninstalling llvmlite-0.43.0:\n",
      "      Successfully uninstalled llvmlite-0.43.0\n",
      "  Attempting uninstall: lark\n",
      "    Found existing installation: lark 1.3.1\n",
      "    Uninstalling lark-1.3.1:\n",
      "      Successfully uninstalled lark-1.3.1\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.60.0\n",
      "    Uninstalling numba-0.60.0:\n",
      "      Successfully uninstalled numba-0.60.0\n",
      "\n",
      "Successfully installed anthropic-0.71.0 apache-tvm-ffi-0.1.8.post2 astor-0.8.1 blake3-1.0.8 cbor2-5.8.0 compressed-tensors-0.12.2 depyf-0.20.0 diskcache-5.6.3 dnspython-2.8.0 email-validator-2.3.0 fastapi-cli-0.0.20 fastapi-cloud-cli-0.11.0 fastar-0.8.0 flashinfer-python-0.5.3 gguf-0.17.1 ijson-3.4.0.post0 interegular-0.3.3 jmespath-1.0.1 lark-1.2.2 llguidance-1.3.0 llvmlite-0.44.0 lm-format-enforcer-0.11.3 loguru-0.7.3 mistral_common-1.8.8 model-hosting-container-standards-0.1.13 msgspec-0.20.0 ninja-1.13.0 numba-0.61.2 nvidia-cudnn-frontend-1.17.0 nvidia-cutlass-dsl-4.3.5 openai-harmony-0.0.8 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post7 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.3 pycountry-24.6.1 pydantic-extra-types-2.11.0 ray-2.53.0 rich-toolkit-0.17.1 rignore-0.7.6 setproctitle-1.3.7 setuptools-80.9.0 supervisor-4.3.0 vllm-0.13.0 xgrammar-0.1.27\n",
      "vLLM: 0.13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "python -m pip install -U pip\n",
    "python -m pip install -U \"vllm\"\n",
    "\n",
    "python -c \"import vllm; print('vLLM:', vllm.__version__)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8681,
     "status": "ok",
     "timestamp": 1768705677916,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "O79S3WnaB4M8",
    "outputId": "656a38eb-b345-4a49-9b36-dc534ae73432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p runs/logs runs/pids runs/ab runs/perf runs/diag\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "VRAM: 40960 MiB\n",
      "python -m pip install -r requirements.txt\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->-r requirements.txt (line 1)) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->-r requirements.txt (line 1)) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 4)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->-r requirements.txt (line 1)) (4.15.0)\n",
      "python -c \"import sys; print(sys.version.split()[0])\"\n",
      "3.12.12\n",
      "python -c \"import torch; print(torch.__version__)\"\n",
      "2.9.0+cu126\n",
      "python -c \"import vllm; print(vllm.__version__)\"\n",
      "0.13.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "make setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129,
     "status": "ok",
     "timestamp": 1768705691583,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "Hr6jurFzB-HX",
    "outputId": "302d7a65-1754-43e3-f37a-1134f52ff6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash scripts/start_base_vllm.sh\n",
      "Expected endpoints:\n",
      "http://0.0.0.0:8000/v1/models\n",
      "http://0.0.0.0:8000/v1/chat/completions\n",
      "Started base vLLM (pid 1363)\n",
      "Logs: /content/slm-hosting-playbook/gemma-slm-hosting/runs/logs/base.log\n",
      "bash scripts/start_ft_vllm.sh\n",
      "Starting FT vLLM\n",
      "HOST=0.0.0.0\n",
      "PORT=8001\n",
      "BASE_MODEL_ID=google/gemma-3-1b-it\n",
      "ADAPTER_PATH=/content/drive/MyDrive/slm-hosting-playbook-artifacts/live_backup/gemma-3-1b-it-lora-20260114-034307\n",
      "Expected endpoints:\n",
      "http://0.0.0.0:8001/v1/models\n",
      "http://0.0.0.0:8001/v1/chat/completions\n",
      "Started FT vLLM (pid 1376)\n",
      "Logs: /content/slm-hosting-playbook/gemma-slm-hosting/runs/logs/ft.log\n",
      "Started base and ft. PIDs in runs/pids/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "make start-both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1768705698052,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "7d0il8xxIoaE",
    "outputId": "20961d82-77b5-49c8-f979-f3a41a27cfee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 data/perf_prompts.jsonl\n",
      "{\"id\":\"p1\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in one sentence.\"}]}\n",
      "{\"id\":\"p2\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain LoRA in 2 short sentences.\"}]}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "mkdir -p data\n",
    "cat > data/perf_prompts.jsonl <<'EOF'\n",
    "{\"id\":\"p1\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello in one sentence.\"}]}\n",
    "{\"id\":\"p2\",\"messages\":[{\"role\":\"user\",\"content\":\"Explain LoRA in 2 short sentences.\"}]}\n",
    "{\"id\":\"p3\",\"messages\":[{\"role\":\"user\",\"content\":\"Give 3 bullets on why GPU is needed for LLM inference.\"}]}\n",
    "{\"id\":\"p4\",\"messages\":[{\"role\":\"user\",\"content\":\"Write a short Python function that adds two numbers.\"}]}\n",
    "{\"id\":\"p5\",\"messages\":[{\"role\":\"user\",\"content\":\"Summarize vLLM in one sentence.\"}]}\n",
    "EOF\n",
    "\n",
    "wc -l data/perf_prompts.jsonl\n",
    "head -n 2 data/perf_prompts.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1768705702696,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "Dd0gjd_TH1u7",
    "outputId": "2a08f807-5c78-4e6a-a5e5-dc628573f5ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== prompts candidates ==\n",
      "./loadtest/workloads/prompts_long.jsonl\n",
      "./loadtest/workloads/prompts_short.jsonl\n",
      "./data/prompts.jsonl\n",
      "./data/perf_prompts.jsonl\n",
      "\n",
      "== list data/ if exists ==\n",
      "total 16\n",
      "drwxr-xr-x  2 root root 4096 Jan 18 03:06 .\n",
      "drwxr-xr-x 16 root root 4096 Jan 18 03:07 ..\n",
      "-rw-r--r--  1 root root  467 Jan 18 03:08 perf_prompts.jsonl\n",
      "-rw-r--r--  1 root root 1218 Jan 18 03:06 prompts.jsonl\n",
      "\n",
      "== smoke_test endpoints it will call (sanity) ==\n",
      "BASE_API_URL= None\n",
      "FT_API_URL= None\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "echo \"== prompts candidates ==\"\n",
    "find . -maxdepth 3 -type f \\( -name \"*prompts*.jsonl\" -o -name \"*prompts*.txt\" -o -name \"*prompts*.json\" \\) -print || true\n",
    "\n",
    "echo\n",
    "echo \"== list data/ if exists ==\"\n",
    "ls -la data 2>/dev/null || true\n",
    "\n",
    "echo\n",
    "echo \"== smoke_test endpoints it will call (sanity) ==\"\n",
    "python -c \"import os; print('BASE_API_URL=', os.getenv('BASE_API_URL')); print('FT_API_URL=', os.getenv('FT_API_URL'))\" || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1768705707507,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "nIzzPjYoICbV",
    "outputId": "6309c676-557b-4a3d-bf65-43320dcff4d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\n",
      "drwxr-xr-x  2 root root 4096 Jan 18 03:08 .\n",
      "drwxr-xr-x 16 root root 4096 Jan 18 03:07 ..\n",
      "-rw-r--r--  1 root root  467 Jan 18 03:08 perf_prompts.jsonl\n",
      "lrwxrwxrwx  1 root root   41 Jan 18 03:08 prompts.jsonl -> ../loadtest/workloads/prompts_short.jsonl\n",
      "{\"prompt\": \"Say hello in one sentence.\"}\n",
      "{\"prompt\": \"List three colors.\"}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "mkdir -p data\n",
    "ln -sf ../loadtest/workloads/prompts_short.jsonl data/prompts.jsonl\n",
    "\n",
    "ls -la data\n",
    "head -n 2 data/prompts.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1768705745444,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "0OSGTO9VLYSP",
    "outputId": "d3c8e7b0-08e9-4a44-a932-d6897fd5965a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_MODEL_ID=google/gemma-3-1b-it\n",
      "ADAPTER_PATH=/content/drive/MyDrive/slm-hosting-playbook-artifacts/live_backup/gemma-3-1b-it-lora-20260114-034307\n",
      "HOST=0.0.0.0\n",
      "BASE_PORT=8000\n",
      "FT_PORT=8001\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "# recreate .env from the repo's example\n",
    "cp -f .env.example .env\n",
    "\n",
    "# sanity\n",
    "grep -E '^(HOST|BASE_PORT|FT_PORT|BASE_MODEL_ID|ADAPTER_PATH)=' .env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81205,
     "status": "ok",
     "timestamp": 1768705830315,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "LLiz1sCeLnSb",
    "outputId": "c815fa08-9547-4180-8060-f1b0659c0913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash scripts/start_base_vllm.sh\n",
      "Expected endpoints:\n",
      "http://0.0.0.0:8000/v1/models\n",
      "http://0.0.0.0:8000/v1/chat/completions\n",
      "Started base vLLM (pid 2289)\n",
      "Logs: /content/slm-hosting-playbook/gemma-slm-hosting/runs/logs/base.log\n",
      "MODE=base\n",
      "models_url=http://127.0.0.1:8000/v1/models\n",
      "chat_url=http://127.0.0.1:8000/v1/chat/completions\n",
      "chat_method=POST\n",
      "chat_model=google/gemma-3-1b-it\n",
      "BASE: Hello to you too! How's your day going so far? 😊 \n",
      "\n",
      "Is there anything you'd like to chat about, or anything I can\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "make stop >/dev/null 2>&1 || true\n",
    "make start-base\n",
    "sleep 10\n",
    "python scripts/smoke_test.py --mode base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91358,
     "status": "ok",
     "timestamp": 1768706196353,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "ZL_rh-NHN0Q6",
    "outputId": "9af977a6-ba25-4888-be73-912a5822b33b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash scripts/start_ft_vllm.sh\n",
      "Starting FT vLLM\n",
      "HOST=0.0.0.0\n",
      "PORT=8001\n",
      "BASE_MODEL_ID=google/gemma-3-1b-it\n",
      "ADAPTER_PATH=/content/drive/MyDrive/slm-hosting-playbook-artifacts/live_backup/gemma-3-1b-it-lora-20260114-034307\n",
      "Expected endpoints:\n",
      "http://0.0.0.0:8001/v1/models\n",
      "http://0.0.0.0:8001/v1/chat/completions\n",
      "Started FT vLLM (pid 4731)\n",
      "Logs: /content/slm-hosting-playbook/gemma-slm-hosting/runs/logs/ft.log\n",
      "MODE=ft\n",
      "models_url=http://127.0.0.1:8001/v1/models\n",
      "chat_url=http://127.0.0.1:8001/v1/chat/completions\n",
      "chat_method=POST\n",
      "chat_model=ft\n",
      "FT: Hello! How can I help you today? 😊\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "# FT only (Colab won't reliably run both)\n",
    "make stop >/dev/null 2>&1 || true\n",
    "make start-ft\n",
    "sleep 15\n",
    "\n",
    "python scripts/smoke_test.py --mode ft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24796,
     "status": "ok",
     "timestamp": 1768706253518,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "0Jb1aS_3OhWB",
    "outputId": "1b88e8fb-2a61-4bea-a73f-999c15802315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url=http://127.0.0.1:8001\n",
      "model=ft\n",
      "concurrency=1\n",
      "total_requests=50\n",
      "success_count=50\n",
      "error_count=0\n",
      "latency_ms_p50=449.83\n",
      "latency_ms_p95=785.02\n",
      "throughput_rps=2.05\n",
      "-rw-r--r-- 1 root root 1.5K Jan 18 03:17 runs/perf/perf_ft.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "mkdir -p runs/perf\n",
    "\n",
    "python bench/perf.py \\\n",
    "  --url http://127.0.0.1:8001/v1 \\\n",
    "  --prompts data/perf_prompts.jsonl \\\n",
    "  --out runs/perf/perf_ft.json\n",
    "\n",
    "ls -lh runs/perf/perf_ft.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1039,
     "status": "ok",
     "timestamp": 1768706272651,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "PkjcOx60QsFi",
    "outputId": "ddbcad2b-adb0-49b4-b59d-71d5fa62529c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash scripts/diag_snapshot.sh\n",
      "Snapshot: /content/slm-hosting-playbook/gemma-slm-hosting/runs/diag/20260118-031750\n",
      "total 20\n",
      "-rw-r--r-- 1 root root 16084 Jan 18 03:17 20260118-031750.tar.gz\n",
      "drwxr-xr-x 2 root root  4096 Jan 18 03:17 20260118-031750\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "make snapshot\n",
    "ls -lt runs/diag | head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1768706282754,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "vw3WaV6GQ62w",
    "outputId": "148a5d47-9b07-4195-e627-6ffaf4913712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash scripts/diag_snapshot.sh\n",
      "Snapshot: /content/slm-hosting-playbook/gemma-slm-hosting/runs/diag/20260118-031801\n",
      "total 40\n",
      "-rw-r--r-- 1 root root 16087 Jan 18 03:18 20260118-031801.tar.gz\n",
      "drwxr-xr-x 2 root root  4096 Jan 18 03:18 20260118-031801\n",
      "-rw-r--r-- 1 root root 16084 Jan 18 03:17 20260118-031750.tar.gz\n",
      "drwxr-xr-x 2 root root  4096 Jan 18 03:17 20260118-031750\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "# Ensure expected log locations exist for snapshot script\n",
    "mkdir -p runs\n",
    "if [ -f runs/logs/base.log ]; then cp -f runs/logs/base.log runs/base.log; fi\n",
    "if [ -f runs/logs/ft.log ]; then cp -f runs/logs/ft.log runs/ft.log; fi\n",
    "\n",
    "make snapshot\n",
    "ls -lt runs/diag | head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1768706287353,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "mvg4dIM3Q86N",
    "outputId": "2254cedb-92f3-4ebf-dbd0-f21e375dbbe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest snapshot: runs/diag/20260118-031801.tar.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "latest=\"$(ls -td runs/diag/20* | head -n 1)\"\n",
    "echo \"Latest snapshot: $latest\"\n",
    "ls -la \"$latest\" | grep -E 'base\\.log|ft\\.log|runs_logs' || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1768706290406,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "hET5mhFjRKgP",
    "outputId": "04d9243e-d492-40b7-87ac-278b04ccea53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== runs/logs ==\n",
      "total 64\n",
      "drwxr-xr-x 2 root root  4096 Jan 18 03:08 .\n",
      "drwxr-xr-x 7 root root  4096 Jan 18 03:18 ..\n",
      "-rw-r--r-- 1 root root 19076 Jan 18 03:15 base.log\n",
      "-rw-r--r-- 1 root root 29543 Jan 18 03:17 ft.log\n",
      "\n",
      "== runs/*.log ==\n",
      "-rw-r--r-- 1 root root 19076 Jan 18 03:18 runs/base.log\n",
      "-rw-r--r-- 1 root root 29543 Jan 18 03:18 runs/ft.log\n",
      "\n",
      "== grep for base/ft logs anywhere under runs ==\n",
      "runs/base.log\n",
      "runs/logs/base.log\n",
      "runs/logs/ft.log\n",
      "runs/ft.log\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "echo \"== runs/logs ==\"\n",
    "ls -la runs/logs 2>/dev/null || true\n",
    "\n",
    "echo\n",
    "echo \"== runs/*.log ==\"\n",
    "ls -la runs/*.log 2>/dev/null || true\n",
    "\n",
    "echo\n",
    "echo \"== grep for base/ft logs anywhere under runs ==\"\n",
    "find runs -maxdepth 3 -type f -name \"*.log\" -print 2>/dev/null || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 936,
     "status": "ok",
     "timestamp": 1768706338805,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "YAmevM_9HNM3",
    "outputId": "0d1fd300-63ef-49d6-b686-22c001796e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash scripts/diag_snapshot.sh\n",
      "Snapshot: /content/slm-hosting-playbook/gemma-slm-hosting/runs/diag/20260118-031857\n",
      "total 60\n",
      "-rw-r--r-- 1 root root 16093 Jan 18 03:18 20260118-031857.tar.gz\n",
      "drwxr-xr-x 2 root root  4096 Jan 18 03:18 20260118-031857\n",
      "-rw-r--r-- 1 root root 16087 Jan 18 03:18 20260118-031801.tar.gz\n",
      "drwxr-xr-x 2 root root  4096 Jan 18 03:18 20260118-031801\n",
      "-rw-r--r-- 1 root root 16084 Jan 18 03:17 20260118-031750.tar.gz\n",
      "drwxr-xr-x 2 root root  4096 Jan 18 03:17 20260118-031750\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "make snapshot\n",
    "ls -lt runs/diag | head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141399,
     "status": "ok",
     "timestamp": 1768706621460,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "HA9tBe1lHPwX",
    "outputId": "e8222d89-e299-4ad4-bcfa-b07d568b6c09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash scripts/start_base_vllm.sh\n",
      "Expected endpoints:\n",
      "http://0.0.0.0:8000/v1/models\n",
      "http://0.0.0.0:8000/v1/chat/completions\n",
      "Started base vLLM (pid 7291)\n",
      "Logs: /content/slm-hosting-playbook/gemma-slm-hosting/runs/logs/base.log\n",
      "[1/120] waiting for BASE... http_code=000\n",
      "[2/120] waiting for BASE... http_code=000\n",
      "[3/120] waiting for BASE... http_code=000\n",
      "[4/120] waiting for BASE... http_code=000\n",
      "[5/120] waiting for BASE... http_code=000\n",
      "[6/120] waiting for BASE... http_code=000\n",
      "[7/120] waiting for BASE... http_code=000\n",
      "[8/120] waiting for BASE... http_code=000\n",
      "[9/120] waiting for BASE... http_code=000\n",
      "[10/120] waiting for BASE... http_code=000\n",
      "[11/120] waiting for BASE... http_code=000\n",
      "[12/120] waiting for BASE... http_code=000\n",
      "[13/120] waiting for BASE... http_code=000\n",
      "[14/120] waiting for BASE... http_code=000\n",
      "[15/120] waiting for BASE... http_code=000\n",
      "[16/120] waiting for BASE... http_code=000\n",
      "[17/120] waiting for BASE... http_code=000\n",
      "[18/120] waiting for BASE... http_code=000\n",
      "[19/120] waiting for BASE... http_code=000\n",
      "[20/120] waiting for BASE... http_code=000\n",
      "[21/120] waiting for BASE... http_code=000\n",
      "[22/120] waiting for BASE... http_code=000\n",
      "[23/120] waiting for BASE... http_code=000\n",
      "[24/120] waiting for BASE... http_code=000\n",
      "[25/120] waiting for BASE... http_code=000\n",
      "[26/120] waiting for BASE... http_code=000\n",
      "[27/120] waiting for BASE... http_code=000\n",
      "[28/120] waiting for BASE... http_code=000\n",
      "[29/120] waiting for BASE... http_code=000\n",
      "[30/120] waiting for BASE... http_code=000\n",
      "[31/120] waiting for BASE... http_code=000\n",
      "✅ BASE ready\n",
      "bash scripts/start_ft_vllm.sh\n",
      "Starting FT vLLM\n",
      "HOST=0.0.0.0\n",
      "PORT=8001\n",
      "BASE_MODEL_ID=google/gemma-3-1b-it\n",
      "ADAPTER_PATH=/content/drive/MyDrive/slm-hosting-playbook-artifacts/live_backup/gemma-3-1b-it-lora-20260114-034307\n",
      "Expected endpoints:\n",
      "http://0.0.0.0:8001/v1/models\n",
      "http://0.0.0.0:8001/v1/chat/completions\n",
      "Started FT vLLM (pid 7870)\n",
      "Logs: /content/slm-hosting-playbook/gemma-slm-hosting/runs/logs/ft.log\n",
      "[1/120] waiting for FT... http_code=000\n",
      "[2/120] waiting for FT... http_code=000\n",
      "[3/120] waiting for FT... http_code=000\n",
      "[4/120] waiting for FT... http_code=000\n",
      "[5/120] waiting for FT... http_code=000\n",
      "[6/120] waiting for FT... http_code=000\n",
      "[7/120] waiting for FT... http_code=000\n",
      "[8/120] waiting for FT... http_code=000\n",
      "[9/120] waiting for FT... http_code=000\n",
      "[10/120] waiting for FT... http_code=000\n",
      "[11/120] waiting for FT... http_code=000\n",
      "[12/120] waiting for FT... http_code=000\n",
      "[13/120] waiting for FT... http_code=000\n",
      "[14/120] waiting for FT... http_code=000\n",
      "[15/120] waiting for FT... http_code=000\n",
      "[16/120] waiting for FT... http_code=000\n",
      "[17/120] waiting for FT... http_code=000\n",
      "[18/120] waiting for FT... http_code=000\n",
      "[19/120] waiting for FT... http_code=000\n",
      "[20/120] waiting for FT... http_code=000\n",
      "[21/120] waiting for FT... http_code=000\n",
      "[22/120] waiting for FT... http_code=000\n",
      "[23/120] waiting for FT... http_code=000\n",
      "[24/120] waiting for FT... http_code=000\n",
      "[25/120] waiting for FT... http_code=000\n",
      "[26/120] waiting for FT... http_code=000\n",
      "[27/120] waiting for FT... http_code=000\n",
      "[28/120] waiting for FT... http_code=000\n",
      "[29/120] waiting for FT... http_code=000\n",
      "[30/120] waiting for FT... http_code=000\n",
      "[31/120] waiting for FT... http_code=000\n",
      "[32/120] waiting for FT... http_code=000\n",
      "[33/120] waiting for FT... http_code=000\n",
      "[34/120] waiting for FT... http_code=000\n",
      "[35/120] waiting for FT... http_code=000\n",
      "[36/120] waiting for FT... http_code=000\n",
      "[37/120] waiting for FT... http_code=000\n",
      "✅ FT ready\n",
      "Saved:\n",
      "-rw-r--r-- 1 root root 932 Jan 18 03:22 runs/ab/base.json\n",
      "-rw-r--r-- 1 root root 979 Jan 18 03:23 runs/ab/ft.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n",
      "curl: (7) Failed to connect to 127.0.0.1 port 8001 after 0 ms: Connection refused\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "PROMPT='Explain LoRA in one sentence, then give one example use-case.'\n",
    "mkdir -p runs/ab\n",
    "\n",
    "wait_ready () {\n",
    "  url=\"$1\"   # e.g. http://127.0.0.1:8000\n",
    "  name=\"$2\"  # BASE/FT\n",
    "  for i in $(seq 1 120); do\n",
    "    code=\"$(curl -sS --max-time 2 -o /dev/null -w \"%{http_code}\" \"${url}/v1/models\" || true)\"\n",
    "    if [ \"$code\" = \"200\" ]; then\n",
    "      echo \"✅ ${name} ready\"\n",
    "      return 0\n",
    "    fi\n",
    "    echo \"[$i/120] waiting for ${name}... http_code=${code}\"\n",
    "    sleep 2\n",
    "  done\n",
    "  echo \"❌ ${name} not ready in time\"\n",
    "  return 1\n",
    "}\n",
    "\n",
    "post_chat () {\n",
    "  url=\"$1\"\n",
    "  model=\"$2\"\n",
    "  out=\"$3\"\n",
    "  curl -sS \"${url}/v1/chat/completions\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d \"{\\\"model\\\":\\\"${model}\\\",\\\"messages\\\":[{\\\"role\\\":\\\"user\\\",\\\"content\\\":\\\"${PROMPT}\\\"}],\\\"max_tokens\\\":80,\\\"temperature\\\":0}\" \\\n",
    "    > \"${out}\"\n",
    "}\n",
    "\n",
    "# ---------- BASE ----------\n",
    "make stop >/dev/null 2>&1 || true\n",
    "make start-base\n",
    "wait_ready \"http://127.0.0.1:8000\" \"BASE\"\n",
    "post_chat \"http://127.0.0.1:8000\" \"google/gemma-3-1b-it\" \"runs/ab/base.json\"\n",
    "make stop >/dev/null 2>&1 || true\n",
    "\n",
    "# ---------- FT ----------\n",
    "make start-ft\n",
    "wait_ready \"http://127.0.0.1:8001\" \"FT\"\n",
    "post_chat \"http://127.0.0.1:8001\" \"ft\" \"runs/ab/ft.json\"\n",
    "\n",
    "echo \"Saved:\"\n",
    "ls -lh runs/ab/*.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1686,
     "status": "ok",
     "timestamp": 1768706684721,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "8YmQnEuKIevO",
    "outputId": "a82c3f7a-43c0-4d1e-e9fb-ea2fb0f2ec52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining vLLM processes:\n",
      "root        6448  6.3  2.1 13258584 1918980 ?    Sl   03:19   0:20 python3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8000 --model google/gemma-3-1b-it --gpu-memory-utilization 0.90 --max-model-len 2048 --max-num-seqs 128 --tensor-parallel-size 1 --dtype auto\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "make stop >/dev/null 2>&1 || true\n",
    "pkill -f \"vllm.entrypoints.openai.api_server\" 2>/dev/null || true\n",
    "pkill -f \"scripts/healthcheck\" 2>/dev/null || true\n",
    "pkill -f \"curl .*8000\" 2>/dev/null || true\n",
    "pkill -f \"curl .*8001\" 2>/dev/null || true\n",
    "\n",
    "echo \"Remaining vLLM processes:\"\n",
    "ps aux | egrep 'vllm|api_server' | grep -v grep || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1768706693211,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "8jJIGe8ZIkIn",
    "outputId": "225197be-4590-466d-ebf1-c1698ae27a27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== BASE output ==\n",
      "google/gemma-3-1b-it\n",
      "LoRA (Low-Rank Adaptation) is a technique that efficiently fine-tunes large language models by only training a small number of additional parameters, significantly reducing computational costs and memory requirements.\n",
      "\n",
      "**Example Use-Case:** Adapting a large language model to generate creative writing prompts for a specific genre.\n",
      "\n",
      "== FT output ==\n",
      "ft\n",
      "LoRA stands for Low-Rank Adaptation, a technique used to fine-tune large language models (LLMs) with limited computational resources.\n",
      "\n",
      "One example use-case is fine-tuning a pre-trained LLM for a specific task, such as sentiment analysis or question answering, using only a small subset of the original model's parameters. This can significantly reduce the computational cost and time required\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "echo \"== BASE output ==\"\n",
    "python - <<'PY'\n",
    "import json\n",
    "d=json.load(open(\"runs/ab/base.json\"))\n",
    "print(d[\"model\"])\n",
    "print(d[\"choices\"][0][\"message\"][\"content\"])\n",
    "PY\n",
    "\n",
    "echo\n",
    "echo \"== FT output ==\"\n",
    "python - <<'PY'\n",
    "import json\n",
    "d=json.load(open(\"runs/ab/ft.json\"))\n",
    "print(d[\"model\"])\n",
    "print(d[\"choices\"][0][\"message\"][\"content\"])\n",
    "PY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1768706706565,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "JM4_2gCGIoJ9",
    "outputId": "3892e60e-4b2f-4b60-acb1-f3053e7f5a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56K\n",
      "-rw-r--r-- 1 root root  932 Jan 18 03:25 base.json\n",
      "-rw-r--r-- 1 root root  18K Jan 18 03:25 base.log\n",
      "-rw-r--r-- 1 root root  979 Jan 18 03:25 ft.json\n",
      "-rw-r--r-- 1 root root  24K Jan 18 03:25 ft.log\n",
      "-rw-r--r-- 1 root root 1.5K Jan 18 03:25 perf_ft.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "mkdir -p runs/blog_bundle\n",
    "cp -f runs/ab/base.json runs/ab/ft.json runs/blog_bundle/\n",
    "\n",
    "# include perf if you already ran it\n",
    "[ -f runs/perf/perf_ft.json ] && cp -f runs/perf/perf_ft.json runs/blog_bundle/ || true\n",
    "[ -f runs/perf/perf_base.json ] && cp -f runs/perf/perf_base.json runs/blog_bundle/ || true\n",
    "\n",
    "# include logs if present\n",
    "[ -f runs/logs/base.log ] && cp -f runs/logs/base.log runs/blog_bundle/ || true\n",
    "[ -f runs/logs/ft.log ] && cp -f runs/logs/ft.log runs/blog_bundle/ || true\n",
    "\n",
    "ls -lh runs/blog_bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1768706774282,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "Q-kEPo-4I3Lt",
    "outputId": "f622426a-3fe7-4386-83c5-dcdb0850fa89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 12K Jan 18 03:26 blog_bundle.zip\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "# zip it (easy to download)\n",
    "cd runs\n",
    "zip -r blog_bundle.zip blog_bundle >/dev/null\n",
    "ls -lh blog_bundle.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1768706873106,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "6-0Bl3mvJQwL",
    "outputId": "0317efce-0a85-4efe-a1b5-a6afef2b52de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56K\n",
      "-rw-r--r-- 1 root root  932 Jan 18 03:25 base.json\n",
      "-rw-r--r-- 1 root root  18K Jan 18 03:25 base.log\n",
      "-rw-r--r-- 1 root root  979 Jan 18 03:25 ft.json\n",
      "-rw-r--r-- 1 root root  24K Jan 18 03:25 ft.log\n",
      "-rw-r--r-- 1 root root 1.5K Jan 18 03:25 perf_ft.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "mkdir -p artifacts/blog_bundle\n",
    "cp -a runs/blog_bundle/. artifacts/blog_bundle/\n",
    "ls -lh artifacts/blog_bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1768707005764,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "hrb8U3oPJxRr",
    "outputId": "77ab2bf0-0be1-47a5-9bd0-439b4338e054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed to: artifacts/poc_run\n",
      "base.json\n",
      "base.log\n",
      "ft.json\n",
      "ft.log\n",
      "perf_ft.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "NEW_DIR=\"artifacts/poc_run\"\n",
    "\n",
    "mkdir -p artifacts\n",
    "rm -rf \"$NEW_DIR\"\n",
    "mv artifacts/blog_bundle \"$NEW_DIR\"\n",
    "\n",
    "echo \"Renamed to: $NEW_DIR\"\n",
    "find \"$NEW_DIR\" -maxdepth 1 -type f -printf \"%f\\n\" | sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1768707040882,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "zFpIJqtjJ53r",
    "outputId": "799c89a1-7466-46c6-af6f-ae904fed61fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoC run evidence (Colab)\n",
      "\n",
      "Files:\n",
      "- base.json, ft.json: A/B prompt responses (BASE vs LoRA FT) from vLLM OpenAI-compatible endpoints.\n",
      "- perf_ft.json: Simple perf numbers captured via bench/perf.py.\n",
      "- base.log, ft.log: vLLM server logs for the run.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "cat > artifacts/poc_run/README.md <<'EOF'\n",
    "PoC run evidence (Colab)\n",
    "\n",
    "Files:\n",
    "- base.json, ft.json: A/B prompt responses (BASE vs LoRA FT) from vLLM OpenAI-compatible endpoints.\n",
    "- perf_ft.json: Simple perf numbers captured via bench/perf.py.\n",
    "- base.log, ft.log: vLLM server logs for the run.\n",
    "EOF\n",
    "\n",
    "sed -n '1,120p' artifacts/poc_run/README.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1768707135774,
     "user": {
      "displayName": "dataarch engineering",
      "userId": "08951085103757294492"
     },
     "user_tz": 360
    },
    "id": "cXj9xC1yKREI",
    "outputId": "12e6ba19-d21a-4893-d3fa-202232955619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: artifacts/poc_run/ (stored 0%)\n",
      "  adding: artifacts/poc_run/README.md (deflated 25%)\n",
      "  adding: artifacts/poc_run/base.log (deflated 81%)\n",
      "  adding: artifacts/poc_run/ft.json (deflated 45%)\n",
      "  adding: artifacts/poc_run/perf_ft.json (deflated 54%)\n",
      "  adding: artifacts/poc_run/ft.log (deflated 77%)\n",
      "  adding: artifacts/poc_run/base.json (deflated 45%)\n",
      "-rw-r--r-- 1 root root 12K Jan 18 03:32 artifacts_poc_run.zip\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/slm-hosting-playbook/gemma-slm-hosting\n",
    "\n",
    "zip -r artifacts_poc_run.zip artifacts/poc_run\n",
    "ls -lh artifacts_poc_run.zip\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNEf94NbO7hgqMFkHRSz6nv",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
